{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9916dccd-b8e5-47b8-bece-d5818e6554f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_wine\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242d996b-7901-474a-afce-e6d888816d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92894261d3a24e8bb487805e15c44d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 4096, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (3): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (4): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (5): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (6): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (7-33): 27 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (34): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "      (35): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da441c49-0b8d-43bd-aa93-4ab7c797b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)           # Should show 2.2.2+cu118\n",
    "print(torch.cuda.is_available())   # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdeba2a-6005-48ad-b1f6-d42eef152a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_json(path_or_buf=\"Data_MT/mt_train.jsonl\", lines=True)\n",
    "data_val = pd.read_json(path_or_buf=\"Data_MT/mt_dev.jsonl\", lines=True)\n",
    "data_test = pd.read_json(path_or_buf=\"Data_MT/mt_test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9767f9a4-3f3d-42b8-a84a-12ab4f3c6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop('context', axis=1)\n",
    "data_val = data_val.drop('context', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e635eb21-3ca8-4c41-8d60-ab2670aab4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我想咨询件事  我对别人有点敌意</td>\n",
       "      <td>ฉันอยากจะถามอะไรหน่อย ฉันรู้สึกไม่เป็นมิตรกับค...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我考虑细菌性或滴虫性阴道炎的可能性</td>\n",
       "      <td>ฉันกำลังคิดว่าน่าจะเกิดภาวะช่องคลอดอักเสบจากเช...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你好！腰痛发现有多长时间了？是腰中间疼痛还是两侧肌肉疼痛？</td>\n",
       "      <td>สวัสดี คุณมีอาการปวดหลังส่วนล่างมานานแค่ไหนแล้...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>是不是平时勤刷牙，多刷干净点就可以减少牙结石的沉积呢？</td>\n",
       "      <td>ถ้าแปรงฟันบ่อยๆ และทำความสะอาดบ่อยๆ ก็จะสามารถ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>周围环境的物理化学因素也可以引起身体过敏</td>\n",
       "      <td>ปัจจัยทางกายภาพและเคมีในสภาพแวดล้อมโดยรอบ ก็สา...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18595</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18596</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18597</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18598</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source  \\\n",
       "0                               我想咨询件事  我对别人有点敌意   \n",
       "1                              我考虑细菌性或滴虫性阴道炎的可能性   \n",
       "2                  你好！腰痛发现有多长时间了？是腰中间疼痛还是两侧肌肉疼痛？   \n",
       "3                    是不是平时勤刷牙，多刷干净点就可以减少牙结石的沉积呢？   \n",
       "4                           周围环境的物理化学因素也可以引起身体过敏   \n",
       "...                                          ...   \n",
       "18595  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18596  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18597  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18598  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18599  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "\n",
       "                                             translation  \n",
       "0      ฉันอยากจะถามอะไรหน่อย ฉันรู้สึกไม่เป็นมิตรกับค...  \n",
       "1      ฉันกำลังคิดว่าน่าจะเกิดภาวะช่องคลอดอักเสบจากเช...  \n",
       "2      สวัสดี คุณมีอาการปวดหลังส่วนล่างมานานแค่ไหนแล้...  \n",
       "3      ถ้าแปรงฟันบ่อยๆ และทำความสะอาดบ่อยๆ ก็จะสามารถ...  \n",
       "4      ปัจจัยทางกายภาพและเคมีในสภาพแวดล้อมโดยรอบ ก็สา...  \n",
       "...                                                  ...  \n",
       "18595  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "18596  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...  \n",
       "18597  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...  \n",
       "18598  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "18599  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "\n",
       "[18600 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3bbc32-163f-47b8-842a-0ea73ecc509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    NllbTokenizer, \n",
    "    M2M100ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Config:\n",
    "    SRC_LANG = \"zho_Hans\"  # Simplified Chinese\n",
    "    TGT_LANG = \"tha_Thai\"  # Thai\n",
    "    # Training hyperparameters (optimized for large dataset)\n",
    "    NUM_EPOCHS = 5       # Reduced epochs for large dataset\n",
    "    BATCH_SIZE = 2       # Reduced for longer sequences\n",
    "    LEARNING_RATE = 5e-5 # Reduced learning rate for stability\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b19ca8-2c61-44b0-8cf8-c30dd47ff276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ตาพร่ามัวและหมดสติเนื่องจากสมองบาดเจ็บ (ชายอาย...\n",
       "1       เดือนที่แล้ว ประจำเดือนครั้งแรกมาตอนต้นเดือน ค...\n",
       "2                  บางคนแพ้ ทำแล้วไม่สบาย อึดอัดและปวดบวม\n",
       "3       ความเครียดเยอะ เบื่ออาหารและน้ำ จะทำให้ผอมลงแล...\n",
       "4              ความรู้สึกตอนนี้คือเหมือนมีอาการประสาทหลอน\n",
       "                              ...                        \n",
       "2995    สวัสดี หินปูนเป็นรอยโรคที่หลงเหลืออยู่หลังจากก...\n",
       "2996            พี่สาวของฉันบอกให้กิน เธอเป็นสูตินรีแพทย์\n",
       "2997          วิธีการรักษาโรคประสาทอ่อน? (ชาย อายุ 31 ปี)\n",
       "2998    หลังการสัมผัสเชื้อมา 3 เดือนสามารถยืนยันได้อย่...\n",
       "2999    ข้อใดข้อหนึ่งก็สามารถตัดความเสี่ยงของการติดเชื...\n",
       "Name: translation, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ead10f-d1ed-433d-afd8-da16165215e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data_val['source'], data_val['translation'], \n",
    "    test_size=0.05,  # 20% for test set\n",
    "    random_state=42,  # for reproducibility\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a15057-32f5-4b89-954e-5d5fce993a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([data_train, pd.DataFrame({\"source\": x_train, \"translation\": y_train})], axis=0)\n",
    "# result = pd.concat([data_train, data_val, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16a7255-6bc8-4019-84a4-eee35c956915",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict({'source': result['source'], 'translation': result['translation']}),\n",
    "    'validation': Dataset.from_dict({'source': x_test, 'translation': y_test}),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f865734-9b3b-4e2d-b45f-876c45875b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'translation'],\n",
       "        num_rows: 21450\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'translation'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5595e94d-1d02-444f-879b-47cc58796086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8195c6-4961-4f55-a9b3-38c0ae5eb954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'translation']\n",
      "<|im_end|> 151645\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"train\"].column_names)\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb19889-5edb-4908-b615-9a69df7946b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['source', 'translation']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099ce9fc-867f-4a78-af69-11e718a11192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf5573d480e41b7a535152684358974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/21450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef38418e40b1463c8e173815973f30f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Prompt template - consistent format\n",
    "    prompts = [f\"翻译成泰语：{src}\\n泰语：\" for src in examples[\"source\"]]\n",
    "    targets = examples[\"translation\"]  # Make sure this matches your column name\n",
    "    \n",
    "    # Create full texts (prompt + target + eos)\n",
    "    full_texts = [prompt + tgt + tokenizer.eos_token for prompt, tgt in zip(prompts, targets)]\n",
    "\n",
    "    # Tokenize full input (prompt + target)\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "        padding='max_length',\n",
    "        return_tensors=None,  # Don't return tensors yet\n",
    "    )\n",
    "\n",
    "    # Tokenize prompts only to get their lengths\n",
    "    prompt_tokenized = tokenizer(\n",
    "        prompts,\n",
    "        add_special_tokens=True,  # IMPORTANT: Must match full tokenization\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "        padding=False,  # Don't pad prompts\n",
    "    )\n",
    "\n",
    "    # Prepare masked labels\n",
    "    labels = []\n",
    "    for i, (input_ids, prompt_ids) in enumerate(zip(tokenized[\"input_ids\"], prompt_tokenized[\"input_ids\"])):\n",
    "        label = input_ids.copy()\n",
    "        prompt_length = len(prompt_ids)\n",
    "        \n",
    "        # Mask the prompt part (set to -100 so it's ignored in loss)\n",
    "        label[:prompt_length] = [-100] * prompt_length\n",
    "        \n",
    "        # Handle padding tokens - also mask them\n",
    "        for j, token_id in enumerate(label):\n",
    "            if token_id == tokenizer.pad_token_id:\n",
    "                label[j] = -100\n",
    "        \n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization to your dataset\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22443674-3a30-45ef-8479-2b10ab41a710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21450\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb39282-46c6-422b-9cb7-f481bfe8195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=32,  # Rank - higher = more parameters but better performance\n",
    "    lora_alpha=32,  # Scaling parameter\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2faaf00-1b7e-4b13-bfb7-2a97b6099018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bdb08a2-226e-4bd9-9d19-5471c09538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir= './Qwen-8B',\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps = 500,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./Qwen-1.7B/logs\",\n",
    "    logging_steps=1000,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,                             # Saves memory; set False if using custom dataset fields\n",
    "    dataloader_pin_memory=True,                             # Helpful on some systems\n",
    "    dataloader_num_workers=2,                               # Speeds up data loading; adjust as needed\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d732a534-2cc4-4942-acbf-b8084a4903af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20292/2915407132.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2348e698-1501-44b6-b797-f1960cadccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 87,293,952\n",
      "Total parameters:     5,279,101,952\n",
      "Trainable %:          1.65%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for param in model.parameters():\n",
    "        total += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable += param.numel()\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters:     {total:,}\")\n",
    "    print(f\"Trainable %:          {100 * trainable / total:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9503aa95-2891-45bf-987e-18a79f87ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5364' max='6705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5364/6705 10:22:07 < 2:35:35, 0.14 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.412000</td>\n",
       "      <td>1.242718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.179300</td>\n",
       "      <td>1.216074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.074700</td>\n",
       "      <td>1.216984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>1.240819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5364, training_loss=1.1476054579175115, metrics={'train_runtime': 37335.8333, 'train_samples_per_second': 2.873, 'train_steps_per_second': 0.18, 'total_flos': 4.492915651141632e+18, 'train_loss': 1.1476054579175115, 'epoch': 4.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6542faf-9dc7-4b00-804e-42c9da3ea2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelx = trainer.model.to(\"cuda\")\n",
    "modelx = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a51947c-8f1b-4747-9d12-4aa00209ec56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 4096, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (3): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (4): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (5): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (6): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (7-33): 27 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (34): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "          (35): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=12288, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20fabf49-82fc-4820-b032-0f57ec303523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(chinese_text):\n",
    "    prompt = f\"翻译成泰语：{chinese_text}\\n泰语：\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "        padding=\"max_length\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Move to device\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        modelx.cuda()\n",
    "    \n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        generated = modelx.generate(\n",
    "            **inputs,\n",
    "            temperature=0.3,  # Lower for more consistent translation\n",
    "            top_p=0.8,\n",
    "            num_beams=10,  # ← This parameter\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample = True,\n",
    "            max_new_tokens=2048,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated part (skip the input prompt)\n",
    "    generated_text = tokenizer.decode(\n",
    "        generated[0][input_length:], \n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad931ca0-6290-4d20-a87c-cc0e8c226635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'อาการตาพร่ามัวและไม่ชัดเจนที่เกิดจากอาการบาดเจ็บที่สมอง (ชายอายุ 40 ปี)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_text(x_test[0]) #'อาการตาพร่ามัวและไม่ชัดเจนที่เกิดจากอาการบาดเจ็บที่สมอง (ชายอายุ 40 ปี)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abcaabd6-f571-42c6-890f-90b6b4a9ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [19:37<00:00,  7.85s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ตอนเช้ามีน้ำมูกสีเหลืองเล็กน้อย แต่ไม่มาก ช่วงอื่นไม่มีน้ำมูกสีเหลือง',\n",
       " 'ถ้าอย่างนั้นก็เป็นโรคเบาหวานอย่างแน่นอน และอาจเป็นก่อนตั้งครรภ์ด้วย',\n",
       " 'ฉันตรวจปัสสาวะเมื่อไม่นานมานี้ พบโปรตีนในปัสสาวะ 1+ และเป็นมาครึ่งเดือนแล้ว',\n",
       " 'งั้นก็ไม่ใช่ปัญหาใหญ่ใช่ไหม',\n",
       " 'ไม่มีอะไรที่มีประสิทธิภาพเป็นพิเศษ ทำตามการตรวจติดตามและตรวจซ้ำ',\n",
       " 'ตอนนี้จะรักษาอย่างไรดี และจะมีชีวิตอยู่ได้นานแค่ไหน?',\n",
       " 'อาการของคุณน่าจะเกิดจากความผิดปกติของระบบประสาทอัตโนมัติ',\n",
       " 'คุณหมอ ฉันคลอดเอง ตอนคลอดฉันรู้สึกเหมือนจะกลับมาเป็นอีก แต่ไม่มีอาการ ไม่ส่งผลต่อเด็กใช่ไหม',\n",
       " 'ด้วยความยินดี หากคุณช่วยได้ โปรดให้คะแนนที่ดี ขอบคุณ!',\n",
       " 'จากคำอธิบายของอัลตราซาวด์ คิดว่าน่าจะเป็นโรคไขมันพอกตับ',\n",
       " 'และอาการหูหนวกเฉียบพลันค่อนข้างยากต่อการรักษา อย่าพนันทั้งหมดไว้กับเขา คุณสามารถรับประทานยานี้ได้ และยังต้องรักษาด้วยการให้น้ำเกลือ',\n",
       " 'การได้ยินของฉันไม่ชัดเจน ไม่ใช่ไม่ได้ยิน',\n",
       " 'ภาวะผมร่วงจากต่อมไขมันอักเสบคืออะไร ใช้ครีมล้างผมอะไรได้บ้าง (เพศหญิง อายุ 20 ปี)',\n",
       " 'ก่อนหน้านี้หมอบอกว่าปากมดลูกสั้นแค่ครึ่งเซนติเมตรและคลายแล้ว',\n",
       " 'สำหรับการวินิจฉัยครั้งนี้ แพทย์ได้อัปเดตคำแนะนำโดยสรุป: โรคนี้เป็นโรคทางพันธุกรรม หากตรวจพบเร็ว ก็ยังสามารถรักษาได้ แต่ตอนนี้สามารถรักษาตามอาการเท่านั้น',\n",
       " 'งั้นเรื่องการรับประทานอาหารทุกมื้อให้สมดุล มีอะไรที่ต้องระวังไหม หรือเรื่องการใช้ชีวิต',\n",
       " 'เคยส่องกล้องตรวจกระเพาะและลำไส้ พบว่าเป็นโรคกระเพาะและมีแผลในกระเพาะอาหาร และมีติ่งเนื้อในลำไส้',\n",
       " 'เนื่องจากมีอาการอาหารไม่ย่อยหรือลำไส้อักเสบ ซึ่งการรักษาทั้งสองอย่างนี้มีความแตกต่างกัน',\n",
       " 'เป็นหวัด เวลาพูดเสียงแหบ น้ำมูกมีเลือดปน เป็นประจำเดือน (เพศหญิง อายุ 37 ปี)',\n",
       " 'หลังผ่าตัดและได้ผลการตรวจทางพยาธิวิทยาแล้ว คุณสามารถปรึกษาฉันได้อีกครั้ง',\n",
       " 'ถ้าทารกในครรภ์มีหมู่เลือดเหมือนกับของฉัน จะทำให้เกิดภาวะเม็ดเลือดแดงแตกใช่ไหม? ถ้าหมู่เลือดเหมือนกับของแม่ก็จะไม่เกิดภาวะเม็ดเลือดแดงแตกใช่ไหม? อัตราส่วนคือ 50% ใช่ไหม ฉันไม่เข้าใจ แค่เดา',\n",
       " 'สิ่งสำคัญคือเธอคิดว่ากินยาพวกนั้นแล้วก็ยังเจ็บอยู่',\n",
       " 'ยาถูกต้อง จำไว้ว่าต้องกินครบ 10 ถึง 14 วัน',\n",
       " 'ช่วงนี้มีอาการใจสั่น หายใจลำบาก และแน่นหน้าอก จะมีความเสี่ยงในการผ่าตัดไหม',\n",
       " 'ส่งผลต่อการทำงานของหลอดเลือด การทำงานของปอด และการทำงานของตับ เป็นต้น',\n",
       " 'ฉันวิ่งมาสัปดาห์หนึ่งแล้ว เริ่มปวดข้างในของเข่าเมื่อสองวันก่อน',\n",
       " 'อาการแบบนี้สามารถตรวจคลื่นไฟฟ้าสมองเพื่อตัดความเป็นไปได้ของการเป็นโรคลมชักได้ใช่ไหม',\n",
       " 'หลังผ่าตัดเนื้องอกไฟโบรแอดิโนมา ควรกินอะไรดีบ้าง ขอบคุณสำหรับคำตอบที่เป็นประโยชน์',\n",
       " 'โรคกระดูกพรุน กระดูกหักง่าย และปวดข้อเท้าขณะวิ่ง (ชาย, 16)',\n",
       " 'ตอนนี้อาการของเขาหนักมาก จะส่งผลต่อชีวิตของเขาในอนาคตหรือไม่?',\n",
       " 'สามารถวินิจฉัยได้ด้วยอัลตราซาวด์สีเท่านั้นว่าแท้งหรือแท้งคุกคามหรือไม่',\n",
       " 'หลังจากมีไข้จากโรคเต้านมอักเสบ สามารถให้นมลูกได้หรือไม่ (เพศหญิง อายุ 38 ปี)',\n",
       " 'ไม่มีใบสั่งยาเหรอ? ซีสต์เนื้อเยื่ออวัยวะมดลูกขนาดเล็กขนาดนี้ สามารถทำให้มีเลือดปนตกขาวได้เฉพาะซีสต์เนื้อเยื่ออวัยวะมดลูกที่อยู่ใต้เยื่อบุโพรงมดลูกเท่านั้น',\n",
       " 'การช่วยตัวเองเมื่อก่อนของฉันทำให้ต่อมลูกหมากโตหรือไม่?',\n",
       " 'อืม ถ้าแปรงด้วยผงแห้งแบบนี้ แล้วขยับข้อไหล่ทุกวัน แบบนี้ก็ยังคงเป็นอาการปวดที่เกิดจากเส้นเอ็นรอบข้อไหล่ตึง',\n",
       " 'ปวดท้องแต่ไม่มีเลือดออก ตั้งครรภ์หรือเปล่า (เพศหญิง อายุ 20 ปี)',\n",
       " 'อายุ 2 ขวบกว่า เขาพูดเองได้แล้ว แต่ยังต้องเรียนรู้อยู่',\n",
       " 'นี่เป็นคำถามสุดท้ายแล้ว ฉันไม่มีคำถามอื่นแล้ว',\n",
       " 'หลังจากทำแท้ง ห้ามมีเพศสัมพันธ์เป็นเวลา 1 เดือน และระวังอย่าให้ร่างกายเย็นเกินไป',\n",
       " 'ใช่ มีเลือดออกแสดงว่ามีริดสีดวงทวาร ควรหลีกเลี่ยงอาหารรสเผ็ดและระคายเคืองด้วย',\n",
       " 'หลังจากสองสัปดาห์ไปอัลตราซาวด์สีเพื่อดูว่าอยู่ในมดลูกหรืออยู่นอกมดลูก',\n",
       " 'ตอนนี้ฉันรู้สึกปวดศีรษะมาก เหมือนจะระเบิด',\n",
       " 'สามารถรับประทานได้จนถึงอายุ 8 เดือน เมื่ออายุ 8 เดือนก็สามารถบดอาหารให้เป็นผงละเอียดได้',\n",
       " 'โดยทั่วไปแล้ว นิ่วในท่อไตสามารถเอาออกได้ แต่ไม่สามารถรับประกันได้ 100%',\n",
       " 'คราวนี้รอถึงเดือนมีนาคมได้ไหม ถ้าประจำเดือนไม่มา ก็ไปตรวจที่โรงพยาบาลใช่ไหม',\n",
       " 'ยาจีนรักษาอาการหย่อนสมรรถภาพทางเพศและหลั่งเร็วได้ผลหรือไม่ (ชายอายุ 48 ปี)',\n",
       " 'หากผลตรวจยังไม่ชัดเจนว่าเป็นชนิดที่ 1 หรือชนิดที่ 2 สามารถตรวจพันธุกรรมเพิ่มเติมได้',\n",
       " 'ไม่เป็นไรค่ะ รบกวนช่วยให้คะแนน 5 ดาวและความคิดเห็นด้วยนะคะ ขอบคุณค่ะ!',\n",
       " 'เป็นยาคุมกำเนิดที่ช่วยปรับประจำเดือนและรักษาโรคถุงน้ำหลายใบ',\n",
       " 'เด็กชาย อายุ 8 ขวบ สายตาเอียงข้างหนึ่งและสายตาสั้นข้างหนึ่ง (ชาย อายุ 8 ขวบ)',\n",
       " 'งั้นลูกอายุแปดเดือนแล้ว คุณได้ตรวจแร่ธาตุแล้วหรือยัง?',\n",
       " 'คุณรู้สึกไม่สบายตรงไหน? แสบร้อน ปากขม คลื่นไส้?',\n",
       " 'เป็นเส้นเลือดกลุ่มหนึ่ง ตรงกลางสามารถสัมผัสได้ถึงเส้นหนึ่งที่ค่อนข้างหนา ปกติจะเป็นอย่างไร?',\n",
       " 'เมื่อฉันไปโรงพยาบาลตอนบ่าย ฉันมีไข้เล็กน้อยและปวดศีรษะ หลังจากกินยาแล้ว ไม่มีอาการปวดศีรษะแล้ว แต่รู้สึกว่าร่างกายร้อนไปทั้งตัว เป็นปฏิกิริยาข้างเคียงจากการกินยาใช่ไหม',\n",
       " 'ช่วงนี้มีใช้ยา ผ่าตัด หรือมีการเปลี่ยนแปลงของน้ำหนักอย่างเห็นได้ชัดหรือไม่?',\n",
       " 'เริ่มมีอาการหูอื้อมา 21 วัน และใช้ยาหยอดหูมา 2 สัปดาห์แล้วไม่ได้ผล',\n",
       " 'อืม โอเค ฉันจะกินยาตามที่หมอสั่งก่อน',\n",
       " 'สายตาสั้นมาก 600 องศา ใส่คอนแทคเลนส์สีได้ไหม (เพศหญิง อายุ 21 ปี)',\n",
       " 'ตอนนี้ฟันผุเจ็บมาก จะทำอย่างไรให้ดีขึ้น (ชาย อายุ 19 ปี)',\n",
       " 'การขับถ่ายดูเหมือนจะไม่ดีนัก ฉันยังไม่กล้าให้เขาทานมากขึ้น ตอนนี้ควรกินอะไรดี? วันที่ 8 หลังผ่าตัด',\n",
       " 'แน่นอนว่าดีกว่านมผง 100 องศา 20 นาที',\n",
       " 'ลูกเป็นโรคปอดบวมและหายแล้ว แต่อาการคัดจมูกรุนแรง จะทำอย่างไร (เพศหญิง อายุ 3 เดือน)',\n",
       " 'ไปตรวจสารก่อภูมิแพ้ก่อน หาสาเหตุสำคัญที่สุด',\n",
       " 'ตอนนี้ฉันยังดีอยู่ ไม่มีอะไรผิดปกติ ทำงานและใช้ชีวิตเป็นปกติ ไม่มีอาการผิดปกติใดๆ และฉันก็ตรวจสุขภาพเป็นประจำที่โรงพยาบาล',\n",
       " 'ติดเชื้อทางเดินปัสสาวะกินยาอะไรดีที่สุด? (เพศหญิง อายุ 32 ปี)',\n",
       " 'ในช่วงสามหรือสี่เดือนที่ผ่านมา ฉันตื่นขึ้นมาสามครั้งตอนกลางดึก ไม่มีอาการเห็นแสงแฟลช และไม่มีอาการอะไรเลยในระหว่างวัน',\n",
       " 'จะทำอย่างไรถ้าเป็นโรคไขข้ออักเสบ? - - - (ชาย, 23)',\n",
       " 'คออักเสบเรื้อรัง น้ำมูกไหลลงคอ เป็นมาเดือนกว่าแล้ว (ชาย อายุ 29 ปี)',\n",
       " 'โรคจมูกอักเสบเรื้อรัง มีเสมหะเยอะควรกินยาอะไร (ชาย อายุ 28 ปี)',\n",
       " 'ระบบทางเดินอาหารของฉันผิดปกติ ฉันควรทำอย่างไร มันส่งผลต่อการเรียน (ชาย อายุ 18 ปี)',\n",
       " 'ฉันไม่ได้กินตลอด ฉันกินเพียงเดือนหนึ่งเท่านั้นเมื่อฉันหยุดยา',\n",
       " 'ฉันตรวจคลื่นไฟฟ้าหัวใจแล้วปกติดี แต่ฉันรู้สึกแน่นหน้าอกเล็กน้อย วันนี้ฉันไปพบแพทย์แผนจีน เขาบอกว่าเลือดไปเลี้ยงไม่เพียงพอ',\n",
       " 'ฉันไปโรงพยาบาลหลายแห่งแล้ว! ฉันกินยาจีนด้วย แต่ไม่ได้ผลเลย',\n",
       " 'หรือไปโรงพยาบาลในพื้นที่และพบแพทย์แผนจีนเพื่อวินิจฉัยและรักษาด้วยยาสมุนไพรจีน และปรับสมดุลร่างกาย',\n",
       " 'โรคอะไรก็ไม่เป็นอันตราย แค่ควบคุมอัตราการเต้นของหัวใจและป้องกันการแข็งตัวของเลือดก็พอ ไม่มีวิธีที่ดีกว่านี้แล้ว',\n",
       " 'ฉันมีอาการท้องเสียและท้องร่วงมา 4 เดือนแล้ว และกินยาหลายชนิด',\n",
       " 'มันขยับไปมา มีก้อนใหญ่ๆ ขยับไปมา แล้วมีจุดดำเล็กๆ มากมาย ดูเหมือนฝน',\n",
       " 'หากคุณปรับตัวได้แล้ว คุณสามารถเปลี่ยนไปใช้ได้เลย',\n",
       " 'ตอนนี้คุณมีอาการอะไรบ้าง? เช่น สายตาเป็นอย่างไร?',\n",
       " 'โอเค ไม่ต้องกินยาได้ใช่ไหม เพราะยาพวกนี้มีผลข้างเคียงด้วย',\n",
       " 'โรคตาแห้งใช้ยาอะไรรักษาได้ดี (เพศหญิง อายุ 24 ปี)',\n",
       " 'คุณหมอ ดูเหมือนว่าฉันจะเป็นโรคคออักเสบจากเริม จะทำอย่างไร (เพศหญิง อายุ 27 ปี)',\n",
       " 'มีน้ำมูกไหลเล็กน้อย น้ำมูกสีเหลือง และมีขี้ตาเยอะในตอนเช้า',\n",
       " 'สวัสดี ยินดีให้คำปรึกษา นานแค่ไหนแล้ว? คุณเคยได้รับการรักษามาก่อนหรือไม่?',\n",
       " 'ในระยะเริ่มต้น ให้ปริมาณยาที่เพียงพอ ปฏิบัติตามขั้นตอนการรักษาอย่างเป็นระบบ มีโอกาสที่จะหายได้ รีบไปรักษาโดยเร็วที่สุด อย่าหยุดยาโดยพลการ ต้องฟังคำแนะนำของแพทย์',\n",
       " 'เมื่อปีที่แล้วส่องกล้องตรวจกระเพาะอาหาร ไม่มีอะไรผิดปกติ แค่กระเพาะอาหารอักเสบแบบไม่เป็นแผล',\n",
       " 'โรคไตมีหลายประเภท การรักษาแต่ละประเภทมีความแตกต่างกัน การตรวจชิ้นเนื้อไตสามารถระบุได้ว่าเป็นประเภทใด และการรักษาที่ดีที่สุดคืออะไร สามารถประเมินการพยากรณ์โรคได้ด้วย',\n",
       " 'โอกาสที่จะไม่ติดเชื้อสูงกว่าโอกาสที่จะติดเชื้อ',\n",
       " 'เนื่องจากเนื้องอกมามาก ทำให้หายช้าใช่ไหมครับคุณหมอ? ผ่านมาสองวันแล้ว ฉันเห็นแม่ของฉันพลิกตัวได้ยากมาก ต้องช่วยเหลือ',\n",
       " 'เมื่อฉันเริ่มรู้สึกไม่สบายตัวครั้งแรก ฉันรู้สึกปวดเกร็งบริเวณหลัง และเมื่อทำบางสิ่งบางอย่างก็รู้สึกเหมือนมีคนกดทับจุดที่เจ็บปวด',\n",
       " 'สวัสดี! โรคหนองในในผู้หญิงมีอาการอะไรบ้าง? (เพศหญิง อายุ 27 ปี)',\n",
       " 'ผู้สูงอายุมักจะมีอาการนี้ ผู้ที่มีสายตาสั้นมากจะมีอาการนี้เร็วขึ้น เกิดจากการเปลี่ยนแปลงของวุ้นตา',\n",
       " 'ถ้ารักษาการติดเชื้อทางเดินปัสสาวะหายแล้ว จะมีโอกาสเป็นโรคไตวายเรื้อรังในอนาคตหรือไม่? หรือจะเป็นโรคไตวายเรื้อรังหลังจากติดเชื้อทางเดินปัสสาวะอีกครั้งในอีกหลายสิบปีหรือไม่?',\n",
       " 'ทางที่ดีควรไปที่แผนกศัลยกรรมทางเดินอาหารเพื่อส่องกล้องตรวจลำไส้ใหญ่เพื่อตัดความเป็นไปได้ที่จะมีเลือดออกมากขนาดนี้',\n",
       " 'ฉันตรวจเลือดเมื่อเดือนสิงหาคม มีค่าหนึ่งที่ผิดปกติ ฉันลืมไปแล้วว่าค่านั้นคืออะไร จากนั้นหมอถามว่าฉันเป็นโรคตับอักเสบบีหรือไม่ ฉันเลยไปตรวจโรคตับอักเสบบีและโรคตับอักเสบ แต่ไม่มีปัญหาอะไร',\n",
       " 'หากอวัยวะเพศแข็งตัวแล้วไม่สามารถโผล่ออกมาได้ทั้งหมด แสดงว่าหนังหุ้มปลายอวัยวะเพศยาวเกินไป หากดึงหนังหุ้มปลายออกด้วยมือแล้วยังไม่เห็นปลายอวัยวะเพศ แสดงว่าเป็นภาวะหนังหุ้มปลายอวัยวะเพศติด ทั้งหนังหุ้มปลายอวัยวะเพศยาวเกินไปและหนังหุ้มปลายอวัยวะเพศติดจำเป็นต้องได้รับการผ่าตัด ขั้นตอนสำคัญคือการดูว่าอวัยวะเพศแข็งตัวแล้ว',\n",
       " 'ผื่นเฉียบพลันในเด็กสามารถเป่าลมและอาบน้ำได้หรือไม่? (ชาย, 1)',\n",
       " 'หากควบคุมไม่ได้ จะส่งผลต่อเนื้อเยื่อและอวัยวะบางส่วน ที่พบบ่อยที่สุดคือปอด',\n",
       " 'สวัสดี หากคุณมีภาวะไตหยางพร่อง คุณสามารถรับประทานยาบำรุงไตและเสริมหยางได้',\n",
       " 'ปกติจะไม่ส่งผลต่อการมองเห็นสิ่งต่างๆ ใดๆ ทั้งสิ้น ตราบใดที่ไม่สังเกตมัน ก็จะมองข้ามมันไป',\n",
       " 'สามารถผ่าตัดแบบแผลเล็กได้ แต่ผลลัพธ์ไม่ดีเท่ากับการผ่าตัดแบบดั้งเดิม เพราะเนื้องอกยังคงอยู่',\n",
       " 'หากหวานเกินไปจะกระตุ้นให้ไอ ดังนั้นจึงแยกไม่ได้ว่าเกิดจากอาการอักเสบหรือการกระตุ้น',\n",
       " 'ผู้ป่วยมะเร็งเม็ดเลือดขาวเรื้อรังสามารถทานยาแก้ปวดได้หรือไม่ (เพศหญิง อายุ 41 ปี)',\n",
       " 'เด็กที่ขาดแคลเซียมหรือวิตามินดีบางครั้งก็เป็นแบบนี้',\n",
       " 'ครรภ์ครั้งแรก หากการเจริญเติบโตของทารกในครรภ์เป็นปกติ แนะนำให้เก็บไว้',\n",
       " 'คำถามได้รับคำตอบแล้ว หากคุณมีคำถามใด ๆ สามารถถามอีกครั้งได้ หากไม่มีปัญหาใด ๆ คุณสามารถให้คะแนน \"พึงพอใจ\" แก่ฉันได้ หน้าการประเมินจะแตกต่างกันไปขึ้นอยู่กับเวอร์ชันของระบบ โดยทั่วไป ปุ่มการประเมินจะปรากฏขึ้นหลังจากปิดคำถามแล้ว ความพึงพอใจของคุณคือกำลังใจและแรงสนับสนุนของฉัน',\n",
       " 'อ่านบทความของคุณแล้ว วิธีทำให้เนื้องอกหยุดเติบโตได้อย่างไร?',\n",
       " 'นอนไม่หลับ นอนหลับยาก ตื่นง่าย (ผู้หญิง, 56)',\n",
       " 'ฉันมีอาการปวดท้องหลังรับประทานอาหาร เป็นโรคกระเพาะหรือแผลไหม?',\n",
       " 'ภาวะไตหยินพร่อง: เวียนศีรษะ หูอื้อ ปวดหลังและเข่า นอนไม่หลับ ฝันมาก ร้อนวูบวาบ เหงื่อออกตอนกลางคืน ร้อนบริเวณมือ เท้า และศีรษะ คอแห้ง แก้มแดง ฟันหลวม ผมร่วง รูปร่างผอม ปัสสาวะสั้นสีเหลืองหรืออุจจาระแข็ง ลิ้นแดงและแห้ง ชีพจรเล็กและเร็ว ผู้ชายจะมีอาการแข็งตัวง่าย ฝันเปียก และหลั่งเร็ว',\n",
       " 'ทางที่ดีควรไปตรวจที่โรงพยาบาลเพื่อตัดความเป็นไปได้ในการเกิดความผิดปกติของอวัยวะออก ถ้าไม่มีอะไรผิดปกติก็แสดงว่าเป็นการทำงานผิดปกติ ซึ่งถือเป็นเรื่องดี แต่ต้องใช้เวลาในการปรับสภาพ และต้องอดทน',\n",
       " 'ผ่าตัดมา 1 เดือนแล้ว ยังมี 10 องศาอยู่',\n",
       " 'สวัสดี ซีสต์ในรังไข่มีหลายประเภท ซีสต์ชนิดหนึ่งที่หลั่งฮอร์โมนเพศสามารถทำให้ประจำเดือนมาไม่ปกติได้ ซึ่งพบได้น้อยมาก',\n",
       " 'โดยทั่วไปสีจะจางลงเมื่ออายุมากขึ้น',\n",
       " 'ฉันเพิ่งเช็ดช่องคลอด ตกขาวมีสีปกติ ตกขาวจะเปลี่ยนสีเมื่อมีเลือดออกหรือไม่?',\n",
       " 'เมื่อคืนฉันไปโรงพยาบาล หมอสั่งยาสวนทวาร 2 ขวด ฉันใช้หมดแล้ว แต่ยังเจ็บอยู่ ฉันจะไปโรงพยาบาลอีกครั้ง คุณหมอ อาจจะอุดตันอีกไหม',\n",
       " 'อาการตาพร่ามัวและไม่ชัดเจนที่เกิดจากอาการบาดเจ็บที่สมอง (ชายอายุ 40 ปี)',\n",
       " 'อย่างไรก็ตาม ยังคงแนะนำให้คุณส่องกล้องโพรงจมูกอีกครั้ง สำหรับอาการแบบนี้ แนะนำให้ส่องกล้องโพรงจมูกทุกปี',\n",
       " 'แนะนำให้พักผ่อนให้มากขึ้นในช่วงไม่กี่วันนี้ และสามารถรับประทานยาเสริมแคลเซียมได้',\n",
       " 'โดยทั่วไป โรควิรัสเริมไม่สามารถรักษาให้หายขาดได้ แต่ผลกระทบต่อผู้ป่วยไม่มากนัก อาการร้อนในก็คือโรควิรัสเริม ถ้ากินพริก ดื่มแอลกอฮอล์มากเกินไป หรือเป็นหวัดแล้วมุมปากบวม ก็คือโรควิรัสเริม แล้วมันจะมีผลกระทบต่อคุณมากแค่ไหน',\n",
       " 'หลังจากรักษาช่องคลอดอักเสบหายแล้ว ปากมดลูกก็จะหายด้วย',\n",
       " 'กินผักและผลไม้ให้มากขึ้นและรักษาการขับถ่ายให้เป็นปกติ',\n",
       " 'สวัสดี! เป็นมานานแค่ไหนแล้ว?',\n",
       " 'วันนี้วัดได้ 37 องศา ยังมีไข้เล็กน้อย',\n",
       " 'แต่ตอนนี้ฉันรู้สึกเวียนหัวและไม่สบาย ฉันสามารถให้นมลูกขณะรับการรักษาแบบนี้ได้หรือไม่?',\n",
       " 'สวัสดี โรคปอดบวมโดยทั่วไปแนะนำให้รักษาด้วยการให้สารน้ำทางหลอดเลือดดำ',\n",
       " 'สวัสดี! คุณเป็นมานานแค่ไหนแล้ว? ตอนนี้มีอาการอะไรบ้าง?',\n",
       " 'บางครั้งฉันมีอาการปัสสาวะบ่อย ปัสสาวะเร่งด่วน และปัสสาวะไม่หมด แต่หลังจากผ่านไปสักพักก็ดีขึ้น',\n",
       " 'สวัสดี! ทารกมีอาการอื่นอีกหรือไม่?',\n",
       " 'กระดูกสันหลังส่วนเอวปลิ้นกดทับเส้นประสาททำให้ปวดขา จะทำอย่างไร (ชายอายุ 63 ปี)',\n",
       " 'อาหารที่ควรหลีกเลี่ยงไม่มากนัก คุณอายุ 40 แล้ว ดื่มน้ำมากขึ้นและกินอาหารอ่อนๆ คุณไม่มีโรคประจำตัวอะไรใช่ไหม',\n",
       " 'เป็นผื่นผิวหนังอักเสบสามารถฉีดวัคซีนป้องกันโรคอีสุกอีใสได้ไหม (เพศหญิง อายุ 18 ปี)',\n",
       " 'คนอายุ 97 ปีที่เป็นโรคหลอดเลือดสมองจะมีชีวิตอยู่ได้นานแค่ไหน (ชาย อายุ 97 ปี)',\n",
       " 'หากเป็นกรณีนี้ แนะนำให้ตรวจเพิ่มเติม การใช้ยาโดยไม่ได้รับการวินิจฉัยจะไม่ได้ผล ถ้าไม่ใช่ยาที่มีประสิทธิภาพต่อการติดเชื้อแบคทีเรีย การใช้ยาจะไม่ได้ผล',\n",
       " 'ถ้าฉันบีบปัสสาวะบ่อยๆ จะทำให้เกิดการติดเชื้อไหม?',\n",
       " 'คุณทำงานเครียดมากในระหว่างวันหรือไม่? คุณมีความเครียดทางจิตใจมากไหม?',\n",
       " 'ความดันโลหิตสูงต้องต่ำกว่า 140 และความดันโลหิตต่ำต้องต่ำกว่า 90 แต่ไม่ต่ำกว่า 60',\n",
       " 'ถ้าเป็นการป้องกัน ก็ทำตามวิธีที่ฉันบอกได้เลย',\n",
       " 'คุณก็คงจะเข้าใจว่าการมีเพศสัมพันธ์นี่ ยิ่งเร่งด่วนยิ่งไม่ดี',\n",
       " 'ฉันเคยแก้ไขฟันมาก่อน แต่รู้สึกว่าแก้ไขไม่ดี',\n",
       " 'หากคุณมีคำถามใด ๆ คุณสามารถปรึกษาได้อีกครั้ง ขอให้ปีลิงมีความสุข',\n",
       " 'มีหลายสาเหตุที่ทำให้ประจำเดือนมาช้า ทั้งอารมณ์ สภาพแวดล้อม อาหาร และสภาพร่างกายก็สามารถทำให้ประจำเดือนมาช้าได้',\n",
       " 'ฉันขอแนะนำให้คุณไปที่แผนกศัลยกรรมทวารหนักของโรงพยาบาลเพื่อตรวจและยืนยันการวินิจฉัย',\n",
       " 'ไม่มีความหมายมากนัก ถ้าเป็นมานาน มันเป็นเนื้อจริงๆ ไม่มียาที่สามารถทำให้มันหายไปได้',\n",
       " 'หลังการผ่าตัด ควรเสริมแคลเซียมและเสริมสร้างการรักษาโรคกระดูกพรุน',\n",
       " 'สวัสดี ทารกคลอดครบกำหนดหรือคลอดก่อนกำหนด?',\n",
       " 'คุณอายุยังน้อยแต่เป็นโรคหลอดเลือดสมอง เกิดจากสาเหตุที่แปลกมาก',\n",
       " 'วิธีที่ดีที่สุดคือให้ฉันดูลิ้นของคุณ แล้วฉันจะช่วยคุณวินิจฉัยได้ หรือคุณสามารถใช้ยาจีนสำเร็จรูปที่บำรุงกระเพาะอาหาร เสริมเลือด และสงบจิตใจได้',\n",
       " 'ตอนที่ฉันบาดเจ็บ ฉันไปตรวจแล้ว กระดูกสันหลังส่วนเอวไม่มีปัญหาอะไร',\n",
       " 'เลือดสีแดงสดหรือเปล่า? เป็นหยดๆ หรือพุ่งออกมา หรือเป็นจุดๆ บนกระดาษ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "predictions = []\n",
    "for text in tqdm.tqdm(x_test):\n",
    "    predictions.append(translate_text(text))\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cefeb91d-9dc8-456a-9529-bd960abe95b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801    ตอนเช้าจะมีน้ำมูกเขียวแต่ไม่มาก เวลาอื่นไม่มีน...\n",
       "1190    แบบนี้แสดงว่าเป็นโรคเบาหวานจริง ๆ และอาจเป็นมา...\n",
       "1817    ช่วงก่อนหน้านี้ ผลตรวจปัสสาวะของฉันพบว่ามีโปรต...\n",
       "251               ถ้าอย่างนั้นก็ไม่มีปัญหาใหญ่อะไรใช่ไหม?\n",
       "2505    ไม่มีอะไรมีสรรพคุณพิเศษ ทำได้แต่ติดตามผลการตรวจไป\n",
       "                              ...                        \n",
       "450           สวัสดี ทารกคลอดตอนครบกำหนดหรือคลอดก่อนกำหนด\n",
       "2164    คุณอายุยังน้อยก็เป็นโรคหลอดเลือดสมองตีบแล้วหรื...\n",
       "2372    ถ้าให้ดี ขอดูภาพลิ้นหน่อย จะได้ช่วยวินิจฉัยได้...\n",
       "2012    ไปตรวจตอนที่ปวดแล้ว กระดูกสันหลังส่วนเอวไม่มีป...\n",
       "270     เลือดเป็นสีแดงสดไหม? เป็นหยดๆ หรือพุ่งออกมา หร...\n",
       "Name: translation, Length: 150, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = []\n",
    "ref = y_test\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e573fd8-56b6-40f1-a39c-43869c976734",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = {\n",
    "    '้ํา' : '้ำ',\n",
    "    '่ํา' : '่ำ',\n",
    "    '๊ํา' : '๊ำ',\n",
    "    '๋ํา' : '๋ำ',\n",
    "    'ํา' : 'ำ',\n",
    "    'เเ' : 'แ'\n",
    "}\n",
    "mem = []\n",
    "for data in predictions:\n",
    "    s = data\n",
    "    for k,v in syn.items(): s = s.replace(k,v)\n",
    "    mem.append(s)\n",
    "predictions = mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d2873c3-75e0-4fa0-9f07-35a51e9019fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 30.03\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import sacrebleu\n",
    "\n",
    "# Tokenize predictions and references\n",
    "tokenized_preds = [\" \".join(word_tokenize(pred)) for pred in predictions]\n",
    "tokenized_refs = [\" \".join(word_tokenize(r)) for r in ref]\n",
    "\n",
    "# Compute BLEU with no tokenization (since we already tokenized)\n",
    "score = sacrebleu.corpus_bleu(tokenized_preds, [tokenized_refs], tokenize=\"none\")\n",
    "print(f\"BLEU score: {score.score:.2f}\")\n",
    "#30.23\n",
    "#29.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f00c722-29c3-446e-ba99-1b682cc081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summit_MT/qwen3-8B(fine-tune)(val).txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in predictions:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b407889-f2cf-49bf-84ca-06035f9280be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "answer = []\n",
    "for text in tqdm.tqdm(data_test['source']):\n",
    "    answer.append(translate_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103e139-9090-4be5-846f-d259ef92bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text(data_test['source'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7fef3-47a6-4713-9228-52e06455a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = []\n",
    "for data in answer:\n",
    "    s = data\n",
    "    for k,v in syn.items(): s = s.replace(k,v)\n",
    "    mem.append(s)\n",
    "answer = mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5095de-83eb-4cc3-8874-03b51c5d6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summit_MT/qwen3-8B(fine-tune).txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in answer:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7209fb-856d-42ce-bf2a-6ff469269346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44e19d-d6f9-4c9c-be14-4772797ec9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f747aff-24ef-413e-9ad2-b24d5d45ed51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b3189-00f2-4a2e-932e-b8ed6d4dd66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271aee51-2bd8-4dfd-9009-4d855533e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3a480-4610-4032-bc66-60df296d562d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
