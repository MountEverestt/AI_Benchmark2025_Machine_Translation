{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9916dccd-b8e5-47b8-bece-d5818e6554f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_wine\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242d996b-7901-474a-afce-e6d888816d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/tokenize\")\n",
    "\n",
    "model = torch.load(\"Qwen/Qwen3-1.7B.pt\", weights_only = False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da441c49-0b8d-43bd-aa93-4ab7c797b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)           # Should show 2.2.2+cu118\n",
    "print(torch.cuda.is_available())   # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdeba2a-6005-48ad-b1f6-d42eef152a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_json(path_or_buf=\"Data_MT/mt_train.jsonl\", lines=True)\n",
    "data_val = pd.read_json(path_or_buf=\"Data_MT/mt_dev.jsonl\", lines=True)\n",
    "data_test = pd.read_json(path_or_buf=\"Data_MT/mt_test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9767f9a4-3f3d-42b8-a84a-12ab4f3c6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop('context', axis=1)\n",
    "data_val = data_val.drop('context', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e635eb21-3ca8-4c41-8d60-ab2670aab4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我想咨询件事  我对别人有点敌意</td>\n",
       "      <td>ฉันอยากจะถามอะไรหน่อย ฉันรู้สึกไม่เป็นมิตรกับค...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我考虑细菌性或滴虫性阴道炎的可能性</td>\n",
       "      <td>ฉันกำลังคิดว่าน่าจะเกิดภาวะช่องคลอดอักเสบจากเช...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你好！腰痛发现有多长时间了？是腰中间疼痛还是两侧肌肉疼痛？</td>\n",
       "      <td>สวัสดี คุณมีอาการปวดหลังส่วนล่างมานานแค่ไหนแล้...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>是不是平时勤刷牙，多刷干净点就可以减少牙结石的沉积呢？</td>\n",
       "      <td>ถ้าแปรงฟันบ่อยๆ และทำความสะอาดบ่อยๆ ก็จะสามารถ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>周围环境的物理化学因素也可以引起身体过敏</td>\n",
       "      <td>ปัจจัยทางกายภาพและเคมีในสภาพแวดล้อมโดยรอบ ก็สา...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18595</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18596</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18597</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18598</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source  \\\n",
       "0                               我想咨询件事  我对别人有点敌意   \n",
       "1                              我考虑细菌性或滴虫性阴道炎的可能性   \n",
       "2                  你好！腰痛发现有多长时间了？是腰中间疼痛还是两侧肌肉疼痛？   \n",
       "3                    是不是平时勤刷牙，多刷干净点就可以减少牙结石的沉积呢？   \n",
       "4                           周围环境的物理化学因素也可以引起身体过敏   \n",
       "...                                          ...   \n",
       "18595  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18596  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18597  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18598  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18599  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "\n",
       "                                             translation  \n",
       "0      ฉันอยากจะถามอะไรหน่อย ฉันรู้สึกไม่เป็นมิตรกับค...  \n",
       "1      ฉันกำลังคิดว่าน่าจะเกิดภาวะช่องคลอดอักเสบจากเช...  \n",
       "2      สวัสดี คุณมีอาการปวดหลังส่วนล่างมานานแค่ไหนแล้...  \n",
       "3      ถ้าแปรงฟันบ่อยๆ และทำความสะอาดบ่อยๆ ก็จะสามารถ...  \n",
       "4      ปัจจัยทางกายภาพและเคมีในสภาพแวดล้อมโดยรอบ ก็สา...  \n",
       "...                                                  ...  \n",
       "18595  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "18596  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...  \n",
       "18597  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...  \n",
       "18598  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "18599  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "\n",
       "[18600 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3bbc32-163f-47b8-842a-0ea73ecc509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    NllbTokenizer, \n",
    "    M2M100ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Config:\n",
    "    SRC_LANG = \"zho_Hans\"  # Simplified Chinese\n",
    "    TGT_LANG = \"tha_Thai\"  # Thai\n",
    "    # Training hyperparameters (optimized for large dataset)\n",
    "    NUM_EPOCHS = 25       # Reduced epochs for large dataset\n",
    "    BATCH_SIZE = 2       # Reduced for longer sequences\n",
    "    LEARNING_RATE = 1e-5 # Reduced learning rate for stability\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b19ca8-2c61-44b0-8cf8-c30dd47ff276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ตาพร่ามัวและหมดสติเนื่องจากสมองบาดเจ็บ (ชายอาย...\n",
       "1       เดือนที่แล้ว ประจำเดือนครั้งแรกมาตอนต้นเดือน ค...\n",
       "2                  บางคนแพ้ ทำแล้วไม่สบาย อึดอัดและปวดบวม\n",
       "3       ความเครียดเยอะ เบื่ออาหารและน้ำ จะทำให้ผอมลงแล...\n",
       "4              ความรู้สึกตอนนี้คือเหมือนมีอาการประสาทหลอน\n",
       "                              ...                        \n",
       "2995    สวัสดี หินปูนเป็นรอยโรคที่หลงเหลืออยู่หลังจากก...\n",
       "2996            พี่สาวของฉันบอกให้กิน เธอเป็นสูตินรีแพทย์\n",
       "2997          วิธีการรักษาโรคประสาทอ่อน? (ชาย อายุ 31 ปี)\n",
       "2998    หลังการสัมผัสเชื้อมา 3 เดือนสามารถยืนยันได้อย่...\n",
       "2999    ข้อใดข้อหนึ่งก็สามารถตัดความเสี่ยงของการติดเชื...\n",
       "Name: translation, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ead10f-d1ed-433d-afd8-da16165215e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data_val['source'], data_val['translation'], \n",
    "    test_size=0.05,  # 20% for test set\n",
    "    random_state=42,  # for reproducibility\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a15057-32f5-4b89-954e-5d5fce993a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([data_train, pd.DataFrame({\"source\": x_train, \"translation\": y_train})], axis=0)\n",
    "# result = pd.concat([data_train, data_val, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16a7255-6bc8-4019-84a4-eee35c956915",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict({'source': result['source'], 'translation': result['translation']}),\n",
    "    'validation': Dataset.from_dict({'source': x_test, 'translation': y_test}),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f865734-9b3b-4e2d-b45f-876c45875b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'translation'],\n",
       "        num_rows: 21450\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'translation'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867ccef5-35be-40d8-a3d3-e46e2fb779fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = config.SRC_LANG\n",
    "tokenizer.tgt_lang = config.TGT_LANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5595e94d-1d02-444f-879b-47cc58796086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba8195c6-4961-4f55-a9b3-38c0ae5eb954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'translation']\n",
      "<|im_end|> 151645\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"train\"].column_names)\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb19889-5edb-4908-b615-9a69df7946b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['source', 'translation']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "099ce9fc-867f-4a78-af69-11e718a11192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23db38815d6d476b8e77ef6c9303992e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b1e9e969d1432c8a64d57131639bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Prompt template\n",
    "    prompts = [f\"将以下中文翻译成泰文：{src}\\n泰文：\" for src in examples[\"source\"]]\n",
    "    targets = examples[\"translation\"]\n",
    "    full_texts = [prompt + tgt + tokenizer.eos_token for prompt, tgt in zip(prompts, targets)]\n",
    "\n",
    "    # Tokenize full input (prompt + target)\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "        padding='max_length',\n",
    "    )\n",
    "\n",
    "    # Tokenize prompts only to get their lengths\n",
    "    prompt_tokenized = tokenizer(\n",
    "        prompts,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "    )\n",
    "\n",
    "    # Prepare masked labels\n",
    "    labels = []\n",
    "    for input_ids, prompt_ids in zip(tokenized[\"input_ids\"], prompt_tokenized[\"input_ids\"]):\n",
    "        label = input_ids.copy()\n",
    "        label[:len(prompt_ids)] = [-100] * len(prompt_ids)\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7154ca6a-a582-40fe-8aca-0be2c23f81a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21450\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb39282-46c6-422b-9cb7-f481bfe8195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=512,  # Rank - higher = more parameters but better performance\n",
    "    lora_alpha=32,  # Scaling parameter\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2faaf00-1b7e-4b13-bfb7-2a97b6099018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bdb08a2-226e-4bd9-9d19-5471c09538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir= './Qwen-1.7B',\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps = 800,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./Qwen-1.7B/logs\",\n",
    "    logging_steps=1000,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,                             # Saves memory; set False if using custom dataset fields\n",
    "    dataloader_pin_memory=True,                             # Helpful on some systems\n",
    "    dataloader_num_workers=2,                               # Speeds up data loading; adjust as needed\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d732a534-2cc4-4942-acbf-b8084a4903af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3254/2915407132.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2348e698-1501-44b6-b797-f1960cadccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 557,842,432\n",
      "Total parameters:     2,278,417,408\n",
      "Trainable %:          24.48%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for param in model.parameters():\n",
    "        total += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable += param.numel()\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters:     {total:,}\")\n",
    "    print(f\"Trainable %:          {100 * trainable / total:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503aa95-2891-45bf-987e-18a79f87ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20095' max='33525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20095/33525 15:18:00 < 10:13:35, 0.36 it/s, Epoch 14.98/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.046200</td>\n",
       "      <td>0.047531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.042629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.039082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.036352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.034497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.033180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.032339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.031621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.031365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.031279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.031251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.031243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.031239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.031315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd9dbbe0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 437d0236-360b-42a4-bbed-028f685fba69)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd9ab130>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: a641ec48-627c-4f93-aaa9-3ea8fbc166b2)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cdc0ef80>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 28ef0b8b-dc17-4fb9-82d3-dda56a37ea7d)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd984250>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 8a97e571-71ef-457e-a669-f9edaa0ee96d)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd337b20>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 9a5113a5-70bc-4825-bc33-d5b814f54459)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd99cd90>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: adcc56d4-662b-4721-b0b7-76b52de0a50c)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd330c70>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 6c6cc0b6-3e00-4650-858a-33509c260054)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cdc0fc40>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: b7d6c817-df7a-4bcd-92ae-24ca10aa5187)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd337e50>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 0c79ae92-cb35-4429-8b7e-f30005f97c10)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cdc0fa90>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 207b60b9-8095-43c1-87a5-cb8d17f003ea)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd31f940>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: c3236b75-d3d8-4442-9543-984316039fb4)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd9f0eb0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: b979caa3-3570-453c-acc5-39354fefe25b)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd9a8100>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 2aac4fc8-f133-458d-8c55-203177dfe845)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-1.7B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f23cd335e70>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 0da289de-c65c-458f-b579-be01b084bfae)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-1.7B.\n",
      "  warnings.warn(\n",
      "/home/tb1017/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-1.7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6542faf-9dc7-4b00-804e-42c9da3ea2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx = trainer.model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51947c-8f1b-4747-9d12-4aa00209ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fabf49-82fc-4820-b032-0f57ec303523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def translate_text(chinese_text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        chinese_text, \n",
    "        truncation=True, \n",
    "        max_length=config.MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        modelx.cuda()\n",
    "        \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = modelx.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=256175,\n",
    "            temperature=0.7\n",
    "            num_beams=10,  # ← This parameter\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode the translation\n",
    "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad931ca0-6290-4d20-a87c-cc0e8c226635",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcaabd6-f571-42c6-890f-90b6b4a9ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "predictions = []\n",
    "for text in tqdm.tqdm(x_test):\n",
    "    predictions.append(translate_text(text))\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefeb91d-9dc8-456a-9529-bd960abe95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = []\n",
    "ref = y_test\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e573fd8-56b6-40f1-a39c-43869c976734",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = {\n",
    "    '้ํา' : '้ำ',\n",
    "    '่ํา' : '่ำ',\n",
    "    '๊ํา' : '๊ำ',\n",
    "    '๋ํา' : '๋ำ',\n",
    "    'ํา' : 'ำ'\n",
    "}\n",
    "mem = []\n",
    "for data in predictions:\n",
    "    s = data\n",
    "    for k,v in syn.items(): s = s.replace(k,v)\n",
    "    mem.append(s)\n",
    "predictions = mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2873c3-75e0-4fa0-9f07-35a51e9019fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import sacrebleu\n",
    "\n",
    "# Tokenize predictions and references\n",
    "tokenized_preds = [\" \".join(word_tokenize(pred)) for pred in predictions]\n",
    "tokenized_refs = [\" \".join(word_tokenize(r)) for r in ref]\n",
    "\n",
    "# Compute BLEU with no tokenization (since we already tokenized)\n",
    "score = sacrebleu.corpus_bleu(tokenized_preds, [tokenized_refs], tokenize=\"none\")\n",
    "print(f\"BLEU score: {score.score:.2f}\")\n",
    "#30.23\n",
    "#29.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00c722-29c3-446e-ba99-1b682cc081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summit_MT/qwen3-1.7B(fine-tune)(val).txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in predictions:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b407889-f2cf-49bf-84ca-06035f9280be",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "for text in tqdm.tqdm(data_test['source']):\n",
    "    answer.append(translate_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103e139-9090-4be5-846f-d259ef92bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text(data_test['source'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7fef3-47a6-4713-9228-52e06455a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = []\n",
    "for data in answer:\n",
    "    s = data\n",
    "    for k,v in syn.items(): s = s.replace(k,v)\n",
    "    mem.append(s)\n",
    "answer = mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5095de-83eb-4cc3-8874-03b51c5d6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summit_MT/qwen3-1.7B(fine-tune).txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in answer:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7209fb-856d-42ce-bf2a-6ff469269346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44e19d-d6f9-4c9c-be14-4772797ec9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f747aff-24ef-413e-9ad2-b24d5d45ed51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b3189-00f2-4a2e-932e-b8ed6d4dd66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271aee51-2bd8-4dfd-9009-4d855533e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3a480-4610-4032-bc66-60df296d562d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
