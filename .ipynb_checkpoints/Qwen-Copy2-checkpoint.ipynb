{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9916dccd-b8e5-47b8-bece-d5818e6554f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_wine\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242d996b-7901-474a-afce-e6d888816d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/tokenize\")\n",
    "\n",
    "model = torch.load(\"Qwen/Qwen3-1.7B.pt\", weights_only = False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da441c49-0b8d-43bd-aa93-4ab7c797b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)           # Should show 2.2.2+cu118\n",
    "print(torch.cuda.is_available())   # Should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdeba2a-6005-48ad-b1f6-d42eef152a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_json(path_or_buf=\"Data_MT/mt_train.jsonl\", lines=True)\n",
    "data_val = pd.read_json(path_or_buf=\"Data_MT/mt_dev.jsonl\", lines=True)\n",
    "data_test = pd.read_json(path_or_buf=\"Data_MT/mt_test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9767f9a4-3f3d-42b8-a84a-12ab4f3c6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop('context', axis=1)\n",
    "data_val = data_val.drop('context', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e635eb21-3ca8-4c41-8d60-ab2670aab4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我想咨询件事  我对别人有点敌意</td>\n",
       "      <td>ฉันอยากจะถามอะไรหน่อย ฉันรู้สึกไม่เป็นมิตรกับค...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我考虑细菌性或滴虫性阴道炎的可能性</td>\n",
       "      <td>ฉันกำลังคิดว่าน่าจะเกิดภาวะช่องคลอดอักเสบจากเช...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你好！腰痛发现有多长时间了？是腰中间疼痛还是两侧肌肉疼痛？</td>\n",
       "      <td>สวัสดี คุณมีอาการปวดหลังส่วนล่างมานานแค่ไหนแล้...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>是不是平时勤刷牙，多刷干净点就可以减少牙结石的沉积呢？</td>\n",
       "      <td>ถ้าแปรงฟันบ่อยๆ และทำความสะอาดบ่อยๆ ก็จะสามารถ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>周围环境的物理化学因素也可以引起身体过敏</td>\n",
       "      <td>ปัจจัยทางกายภาพและเคมีในสภาพแวดล้อมโดยรอบ ก็สา...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18595</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18596</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18597</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18598</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>[自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。</td>\n",
       "      <td>[ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source  \\\n",
       "0                               我想咨询件事  我对别人有点敌意   \n",
       "1                              我考虑细菌性或滴虫性阴道炎的可能性   \n",
       "2                  你好！腰痛发现有多长时间了？是腰中间疼痛还是两侧肌肉疼痛？   \n",
       "3                    是不是平时勤刷牙，多刷干净点就可以减少牙结石的沉积呢？   \n",
       "4                           周围环境的物理化学因素也可以引起身体过敏   \n",
       "...                                          ...   \n",
       "18595  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18596  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18597  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18598  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "18599  [自动回复]您好，现在比较忙，请您留言具体问题，稍后我会及时回复您的问题，请谅解。   \n",
       "\n",
       "                                             translation  \n",
       "0      ฉันอยากจะถามอะไรหน่อย ฉันรู้สึกไม่เป็นมิตรกับค...  \n",
       "1      ฉันกำลังคิดว่าน่าจะเกิดภาวะช่องคลอดอักเสบจากเช...  \n",
       "2      สวัสดี คุณมีอาการปวดหลังส่วนล่างมานานแค่ไหนแล้...  \n",
       "3      ถ้าแปรงฟันบ่อยๆ และทำความสะอาดบ่อยๆ ก็จะสามารถ...  \n",
       "4      ปัจจัยทางกายภาพและเคมีในสภาพแวดล้อมโดยรอบ ก็สา...  \n",
       "...                                                  ...  \n",
       "18595  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "18596  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...  \n",
       "18597  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้หมอค่อนข้างยุ่...  \n",
       "18598  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "18599  [ตอบกลับอัตโนมัติ] สวัสดี ตอนนี้ฉันค่อนข้างยุ่...  \n",
       "\n",
       "[18600 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3bbc32-163f-47b8-842a-0ea73ecc509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    NllbTokenizer, \n",
    "    M2M100ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Config:\n",
    "    SRC_LANG = \"zho_Hans\"  # Simplified Chinese\n",
    "    TGT_LANG = \"tha_Thai\"  # Thai\n",
    "    # Training hyperparameters (optimized for large dataset)\n",
    "    NUM_EPOCHS = 20       # Reduced epochs for large dataset\n",
    "    BATCH_SIZE = 2       # Reduced for longer sequences\n",
    "    LEARNING_RATE = 1e-5 # Reduced learning rate for stability\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b19ca8-2c61-44b0-8cf8-c30dd47ff276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ตาพร่ามัวและหมดสติเนื่องจากสมองบาดเจ็บ (ชายอาย...\n",
       "1       เดือนที่แล้ว ประจำเดือนครั้งแรกมาตอนต้นเดือน ค...\n",
       "2                  บางคนแพ้ ทำแล้วไม่สบาย อึดอัดและปวดบวม\n",
       "3       ความเครียดเยอะ เบื่ออาหารและน้ำ จะทำให้ผอมลงแล...\n",
       "4              ความรู้สึกตอนนี้คือเหมือนมีอาการประสาทหลอน\n",
       "                              ...                        \n",
       "2995    สวัสดี หินปูนเป็นรอยโรคที่หลงเหลืออยู่หลังจากก...\n",
       "2996            พี่สาวของฉันบอกให้กิน เธอเป็นสูตินรีแพทย์\n",
       "2997          วิธีการรักษาโรคประสาทอ่อน? (ชาย อายุ 31 ปี)\n",
       "2998    หลังการสัมผัสเชื้อมา 3 เดือนสามารถยืนยันได้อย่...\n",
       "2999    ข้อใดข้อหนึ่งก็สามารถตัดความเสี่ยงของการติดเชื...\n",
       "Name: translation, Length: 3000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ead10f-d1ed-433d-afd8-da16165215e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data_val['source'], data_val['translation'], \n",
    "    test_size=0.05,  # 20% for test set\n",
    "    random_state=42,  # for reproducibility\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a15057-32f5-4b89-954e-5d5fce993a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([data_train, pd.DataFrame({\"source\": x_train, \"translation\": y_train})], axis=0)\n",
    "# result = pd.concat([data_train, data_val, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16a7255-6bc8-4019-84a4-eee35c956915",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict({'source': result['source'], 'translation': result['translation']}),\n",
    "    'validation': Dataset.from_dict({'source': x_test, 'translation': y_test}),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f865734-9b3b-4e2d-b45f-876c45875b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'translation'],\n",
       "        num_rows: 21450\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'translation'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867ccef5-35be-40d8-a3d3-e46e2fb779fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.src_lang = config.SRC_LANG\n",
    "tokenizer.tgt_lang = config.TGT_LANG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5595e94d-1d02-444f-879b-47cc58796086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba8195c6-4961-4f55-a9b3-38c0ae5eb954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'translation']\n",
      "<|im_end|> 151645\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"train\"].column_names)\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb19889-5edb-4908-b615-9a69df7946b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['source', 'translation']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "099ce9fc-867f-4a78-af69-11e718a11192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd53f97a9cc4cb186239a0290286e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/21450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d9b6582b72469bb48d1bd8479c6b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Prompt template - consistent format\n",
    "    prompts = [f\"翻译成泰语：{src}\\n泰语：\" for src in examples[\"source\"]]\n",
    "    targets = examples[\"translation\"]  # Make sure this matches your column name\n",
    "    \n",
    "    # Create full texts (prompt + target + eos)\n",
    "    full_texts = [prompt + tgt + tokenizer.eos_token for prompt, tgt in zip(prompts, targets)]\n",
    "\n",
    "    # Tokenize full input (prompt + target)\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "        padding='max_length',\n",
    "        return_tensors=None,  # Don't return tensors yet\n",
    "    )\n",
    "\n",
    "    # Tokenize prompts only to get their lengths\n",
    "    prompt_tokenized = tokenizer(\n",
    "        prompts,\n",
    "        add_special_tokens=True,  # IMPORTANT: Must match full tokenization\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "        padding=False,  # Don't pad prompts\n",
    "    )\n",
    "\n",
    "    # Prepare masked labels\n",
    "    labels = []\n",
    "    for i, (input_ids, prompt_ids) in enumerate(zip(tokenized[\"input_ids\"], prompt_tokenized[\"input_ids\"])):\n",
    "        label = input_ids.copy()\n",
    "        prompt_length = len(prompt_ids)\n",
    "        \n",
    "        # Mask the prompt part (set to -100 so it's ignored in loss)\n",
    "        label[:prompt_length] = [-100] * prompt_length\n",
    "        \n",
    "        # Handle padding tokens - also mask them\n",
    "        for j, token_id in enumerate(label):\n",
    "            if token_id == tokenizer.pad_token_id:\n",
    "                label[j] = -100\n",
    "        \n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization to your dataset\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7154ca6a-a582-40fe-8aca-0be2c23f81a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21450\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb39282-46c6-422b-9cb7-f481bfe8195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=512,  # Rank - higher = more parameters but better performance\n",
    "    lora_alpha=32,  # Scaling parameter\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2faaf00-1b7e-4b13-bfb7-2a97b6099018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bdb08a2-226e-4bd9-9d19-5471c09538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir= './Qwen-1.7B',\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps = 800,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./Qwen-1.7B/logs\",\n",
    "    logging_steps=1000,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,                             # Saves memory; set False if using custom dataset fields\n",
    "    dataloader_pin_memory=True,                             # Helpful on some systems\n",
    "    dataloader_num_workers=2,                               # Speeds up data loading; adjust as needed\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d732a534-2cc4-4942-acbf-b8084a4903af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27902/2915407132.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2348e698-1501-44b6-b797-f1960cadccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 557,842,432\n",
      "Total parameters:     2,278,417,408\n",
      "Trainable %:          24.48%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for param in model.parameters():\n",
    "        total += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable += param.numel()\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters:     {total:,}\")\n",
    "    print(f\"Trainable %:          {100 * trainable / total:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503aa95-2891-45bf-987e-18a79f87ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='26820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   59/26820 02:34 < 20:08:34, 0.37 it/s, Epoch 0.04/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6542faf-9dc7-4b00-804e-42c9da3ea2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx = trainer.model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51947c-8f1b-4747-9d12-4aa00209ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fabf49-82fc-4820-b032-0f57ec303523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(chinese_text):\n",
    "    prompt = f\"翻译成泰语：{chinese_text}\\n泰语：\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1140,\n",
    "        padding=\"max_length\",\n",
    "    ).to(\"cude\")\n",
    "    \n",
    "    # Move to device\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        modelx.cuda()\n",
    "    \n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        generated = modelx.generate(\n",
    "            **inputs,\n",
    "            temperature=0.7,  # Lower for more consistent translation\n",
    "            do_sample=True,\n",
    "            top_p=0.8,\n",
    "            num_beams=10,  # ← This parameter\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated part (skip the input prompt)\n",
    "    generated_text = tokenizer.decode(\n",
    "        generated[0][input_length:], \n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad931ca0-6290-4d20-a87c-cc0e8c226635",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcaabd6-f571-42c6-890f-90b6b4a9ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "predictions = []\n",
    "for text in tqdm.tqdm(x_test):\n",
    "    predictions.append(translate_text(text))\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefeb91d-9dc8-456a-9529-bd960abe95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = []\n",
    "ref = y_test\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e573fd8-56b6-40f1-a39c-43869c976734",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = {\n",
    "    '้ํา' : '้ำ',\n",
    "    '่ํา' : '่ำ',\n",
    "    '๊ํา' : '๊ำ',\n",
    "    '๋ํา' : '๋ำ',\n",
    "    'ํา' : 'ำ'\n",
    "}\n",
    "mem = []\n",
    "for data in predictions:\n",
    "    s = data\n",
    "    for k,v in syn.items(): s = s.replace(k,v)\n",
    "    mem.append(s)\n",
    "predictions = mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2873c3-75e0-4fa0-9f07-35a51e9019fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import sacrebleu\n",
    "\n",
    "# Tokenize predictions and references\n",
    "tokenized_preds = [\" \".join(word_tokenize(pred)) for pred in predictions]\n",
    "tokenized_refs = [\" \".join(word_tokenize(r)) for r in ref]\n",
    "\n",
    "# Compute BLEU with no tokenization (since we already tokenized)\n",
    "score = sacrebleu.corpus_bleu(tokenized_preds, [tokenized_refs], tokenize=\"none\")\n",
    "print(f\"BLEU score: {score.score:.2f}\")\n",
    "#30.23\n",
    "#29.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00c722-29c3-446e-ba99-1b682cc081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summit_MT/qwen3-1.7B(fine-tune-linear)(val).txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in predictions:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b407889-f2cf-49bf-84ca-06035f9280be",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "for text in tqdm.tqdm(data_test['source']):\n",
    "    answer.append(translate_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103e139-9090-4be5-846f-d259ef92bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text(data_test['source'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7fef3-47a6-4713-9228-52e06455a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = []\n",
    "for data in answer:\n",
    "    s = data\n",
    "    for k,v in syn.items(): s = s.replace(k,v)\n",
    "    mem.append(s)\n",
    "answer = mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5095de-83eb-4cc3-8874-03b51c5d6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summit_MT/qwen3-1.7B(fine-tune-linear).txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in answer:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7209fb-856d-42ce-bf2a-6ff469269346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44e19d-d6f9-4c9c-be14-4772797ec9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f747aff-24ef-413e-9ad2-b24d5d45ed51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b3189-00f2-4a2e-932e-b8ed6d4dd66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271aee51-2bd8-4dfd-9009-4d855533e398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3a480-4610-4032-bc66-60df296d562d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
